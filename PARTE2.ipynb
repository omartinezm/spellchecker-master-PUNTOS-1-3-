{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import ascii_lowercase\n",
    "\n",
    "\n",
    "\n",
    "def fetch_words(read_mode):\n",
    "    '''Función no alterda por el ataque'''\n",
    "    '''Agregué la opción `enconding=\"utf8\"`'''\n",
    "\n",
    "    words_from_dictionary = [ word.strip() for word in open('words.txt', encoding=\"utf8\").readlines() ]\n",
    "    words_from_books = re.findall(r'\\w+', open('BOOKS.txt', read_mode, encoding=\"utf8\").read())\n",
    "    \n",
    "    return words_from_dictionary + words_from_books\n",
    "\n",
    "# El archivo se había abierto como escritura, pero se debe abrir en modo lectura.\n",
    "WORDS = fetch_words('r') #<<<---\n",
    "LETTERS = list(ascii_lowercase) + ['ñ', 'á', 'é', 'í', 'ó', 'ú']#+[',','?','¿']\n",
    "SYMBOLS = ',.-¡!¿?#$@&%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80400"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(WORDS_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ñ', 'á', 'é', 'í', 'ó', 'ú']\n"
     ]
    }
   ],
   "source": [
    "print(LETTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un índice de palabras en el cual guardamos la frecuencia con la que se utiliza en nuestros\n",
    "# textos de referencia\n",
    "WORDS_INDEX = {}\n",
    "\n",
    "# Las ordenes estaban mal y/o al reves.\n",
    "for word in WORDS:\n",
    "    if word in WORDS_INDEX:\n",
    "        WORDS_INDEX.update({word:WORDS_INDEX[word]+ 1})\n",
    "    else:\n",
    "        WORDS_INDEX[word] = 1\n",
    "\n",
    "for symbol in SYMBOLS:\n",
    "    WORDS_INDEX[symbol]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_real_words(words):\n",
    "    return set(word for word in words if word.lower() in WORDS_INDEX) #Agregué el lowecase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_real_words(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_length_edit(word):\n",
    "    '''Función no alterda por el ataque'''\n",
    "    \n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    #print(splits)\n",
    "    removals_of_one_letter = []\n",
    "    \n",
    "    for left, right in splits:\n",
    "        if right:\n",
    "            removals_of_one_letter.append(left + right[1:])\n",
    "            \n",
    "    two_letters_transposes = []\n",
    "    \n",
    "    for left, right in splits:\n",
    "        if len(right) > 1:\n",
    "            two_letters_transposes.append(left + right[1] + right[0] + right[2:])\n",
    "            \n",
    "    one_letter_replaces = []\n",
    "    \n",
    "    for left, right in splits:\n",
    "        if right:\n",
    "            for c in LETTERS:\n",
    "                one_letter_replaces.append(left + c + right[1:])\n",
    "                \n",
    "    one_letter_insertions = []\n",
    "    \n",
    "    for left, right in splits:\n",
    "        for c in LETTERS:\n",
    "            one_letter_insertions.append(left + c + right)\n",
    "    \n",
    "    one_length_editions = removals_of_one_letter + two_letters_transposes + one_letter_replaces + one_letter_insertions\n",
    "    \n",
    "    return list(set(one_length_editions))\n",
    "\n",
    "\n",
    "\n",
    "def two_lenght_edit(word):\n",
    "    '''Función no alterda por el ataque'''\n",
    "    return [e2 for e1 in one_length_edit(word) for e2 in one_length_edit(e1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def possible_corrections(word):\n",
    "    cap=word[0].isupper()\n",
    "    lword=word.lower()\n",
    "    single_word_possible_corrections = filter_real_words([lword])\n",
    "    one_length_edit_possible_corrections = filter_real_words(one_length_edit(lword))\n",
    "    two_lenght_edit_possible_corrections = filter_real_words(two_lenght_edit(lword))\n",
    "    no_correction_at_all = word\n",
    "    ## Estaban trocados el primer y el tercer caso.\n",
    "    if single_word_possible_corrections:\n",
    "        if cap:\n",
    "            return {option.title() for option in single_word_possible_corrections}\n",
    "        return single_word_possible_corrections\n",
    "    \n",
    "    elif one_length_edit_possible_corrections:\n",
    "        if cap:\n",
    "            return {option.title() for option in one_length_edit_possible_corrections}\n",
    "        return one_length_edit_possible_corrections\n",
    "    \n",
    "    elif two_lenght_edit_possible_corrections:\n",
    "        if cap:\n",
    "            return {option.title() for option in two_lenght_edit_possible_corrections}\n",
    "        return two_lenght_edit_possible_corrections\n",
    "    \n",
    "    else:\n",
    "        return no_correction_at_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Él'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_corrections(\"Él\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check_word(word):\n",
    "    cap=word[0].isupper()\n",
    "    if not cap:\n",
    "        return max(possible_corrections(word.lower()), key=language_model)\n",
    "    else:\n",
    "        return max(possible_corrections(word.lower()), key=language_model).title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model(word):\n",
    "    return WORDS_INDEX.get(word, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_split(sentence):\n",
    "    \"\"\" Esta función crea una lista que divide la frase ubicando los signos aparte\n",
    "        de las palabras\n",
    "        \n",
    "        Parametros\n",
    "        ----------\n",
    "        sentence: str\n",
    "                La frase que se desea partir\n",
    "    \"\"\"\n",
    "    stripped = list(map(lambda x : x, sentence.split()))\n",
    "    stripped_sentence=[]\n",
    "    for word in stripped:\n",
    "        if any(car in word for car in \",.?¿¡!\"):\n",
    "            stripped_sentence.extend(word_split(word))\n",
    "        else:\n",
    "            stripped_sentence.append(word)\n",
    "    return stripped_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_split(word):\n",
    "    punt=SYMBOLS\n",
    "    if not any(p in word for p in punt):\n",
    "        return [word]\n",
    "    tw=word\n",
    "    carry=[]\n",
    "    while(len(punt)>0):\n",
    "        if(tw.find(punt[0])!=-1):\n",
    "            ri=tw[:tw.find(punt[0])]\n",
    "            c=tw[tw.find(punt[0])]\n",
    "            rd=tw[tw.find(punt[0])+1:]\n",
    "            if(len(ri)!=0):\n",
    "                carry.extend(word_split(ri))\n",
    "            carry.append(c)\n",
    "            if(len(rd)!=0):\n",
    "                carry.extend(word_split(rd))\n",
    "            break\n",
    "        punt=punt[1:]\n",
    "    return carry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_join(words):\n",
    "    \"\"\" Esta función crea la frase con el formato correcto de las palabras\n",
    "    \n",
    "        Parametro\n",
    "        ---------\n",
    "        words: list\n",
    "             Las palabras que se desean conectar, puede contener signos de puntuación\n",
    "    \"\"\"\n",
    "    sentence=\"\"\n",
    "    for i in range(len(words)):\n",
    "        if i==0:\n",
    "            if not any(car in words[i] for car in \",.?¿¡!\"):\n",
    "                sentence+=words[i]\n",
    "        elif any(car in words[i] for car in \",.?¿¡!\"):\n",
    "            sentence+=words[i]\n",
    "        elif len(sentence)==0:\n",
    "            sentence+=words[i]\n",
    "        else:\n",
    "            sentence+=\" \"+words[i]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def spell_check_sentence(sentence):\n",
    "    stripped_sentence = correct_split(sentence)#list(map(lambda x : x, lower_cased_sentence.split()))\n",
    "    checked = map(spell_check_word, stripped_sentence) #<<-- Ojo que cambié ese filter por map\n",
    "    checked_words=list(checked)\n",
    "    return correct_join(checked_words)#' '.join(checked_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Él, no era una persona de fiar pues era un mentiroso'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_check_sentence(\".Él, no era una persona de fiar pues era un mentirozo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el Arrebol del día\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-84b5dcd81dc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspell_check_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[1;34m'él no era una persona de fiar pues era un mentiroso'\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mspell_check_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mtest_spell_check_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-84b5dcd81dc1>\u001b[0m in \u001b[0;36mtest_spell_check_sentence\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'el Arebol del día'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspell_check_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[1;34m'el arrebol del día'\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mspell_check_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Rezpeto por la educasión'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_spell_check_sentence():\n",
    "\n",
    "    sentence = 'fabor guardar cilencio para no molestar'\n",
    "    assert 'favor guardar silencio para no molestar' == spell_check_sentence(sentence) \n",
    "\n",
    "    \n",
    "    sentence = 'un lgar para la hopinion'\n",
    "    #no ha lugar para la opinión\n",
    "    assert 'un lugar para la opinión' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'el Arebol del día'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'el arrebol del día' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'Rezpeto por la educasión'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'respeto por la educación' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'RTe encanta conduzir'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'te encanta conducir' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'HOy ay karne azada frezca siga pa dentro'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'hoy ay carne azada fresca siga la dentro' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'En mi ezcuela no enseñan a escrivir ni a ler'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'en mi escuela no enseñan a escribir ni a le' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'él no era una persona de fiar pues era un mentirozo'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'él no era una persona de fiar pues era un mentiroso' == spell_check_sentence(sentence)\n",
    "test_spell_check_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_corrections(\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_INDEX[\"atar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_INDEX[\"matar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_corrections(\"hatar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Solución 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el arrebol del día\n",
      "respeto por la educación\n",
      "te encanta conducir\n",
      "hoy ay carne azada fresca siga la dentro\n",
      "en mi escuela no enseñan a escribir ni a le\n",
      "él no era una persona de fiar pues era un mentiroso\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import ascii_lowercase\n",
    "\n",
    "\n",
    "\n",
    "def fetch_words(read_mode):\n",
    "    '''Función no alterda por el ataque\n",
    "       ---\n",
    "       Se agregó la opción enconding=\"utf8\" en la importación de cada archivo.\n",
    "       De esta forma se evitan problemas de codificación.\n",
    "    '''\n",
    "    words_from_dictionary = [ word.strip() for word in open('words.txt', encoding=\"utf8\").readlines() ]\n",
    "    words_from_books = re.findall(r'\\w+', open('BOOKS.txt', read_mode, encoding=\"utf8\").read())\n",
    "    return words_from_dictionary + words_from_books\n",
    "\n",
    "# Abrimos el archivos en modo lectura y creamos el archivo de palabras y letras.\n",
    "WORDS = fetch_words('r') \n",
    "LETTERS = list(ascii_lowercase) + ['ñ', 'á', 'é', 'í', 'ó', 'ú']\n",
    "\n",
    "# Definimos un índice de palabras en el cual guardamos la frecuencia con la que se utiliza en nuestros\n",
    "# textos de referencia. Esto nos será útil a la hora de \"medir\" la distancia entre una palabra mal\n",
    "# escrita y un candidato.\n",
    "WORDS_INDEX = {}\n",
    "for word in WORDS:\n",
    "    if word in WORDS_INDEX:\n",
    "        WORDS_INDEX.update({word:WORDS_INDEX[word]+ 1}) # Si la palabra ya está aumentamos su frecuencia\n",
    "    else:\n",
    "        WORDS_INDEX[word] = 1 # Si la palabra no está la agregamos\n",
    "\n",
    "def possible_corrections(word):\n",
    "    ''' Busca las posibles correcciones que podemos hacer a una palabra\n",
    "    \n",
    "        El parámetro de búsqueda es el siguiente:\n",
    "        1. Primero se revisa si la palabra pertenece al diccionario,\n",
    "        2. Luego se miran las posibles palabras a distancia 1,\n",
    "        3. Luego se miran las posibles palabras a distancia 2\n",
    "        \n",
    "        En caso tal que todo esto falle se devuelve la palabra sin cambio alguno\n",
    "        \n",
    "        Parámetro\n",
    "        ---------\n",
    "        word : str\n",
    "             Palabra a consultar\n",
    "    '''\n",
    "    single_word_possible_corrections = filter_real_words([word])\n",
    "    one_length_edit_possible_corrections = filter_real_words(one_length_edit(word))\n",
    "    two_lenght_edit_possible_corrections = filter_real_words(two_lenght_edit(word))\n",
    "    no_correction_at_all = word\n",
    "    \n",
    "    if single_word_possible_corrections:\n",
    "        # Si la palabra pertenece al diccionario.\n",
    "        return single_word_possible_corrections\n",
    "    \n",
    "    elif one_length_edit_possible_corrections:\n",
    "        # Si hay correcciones de distancia 1\n",
    "        return one_length_edit_possible_corrections\n",
    "    \n",
    "    elif two_lenght_edit_possible_corrections:\n",
    "        # Si hay correcciones de diatancia 2\n",
    "        return two_lenght_edit_possible_corrections\n",
    "    \n",
    "    else:\n",
    "        # En cualquier otro caso se devuelve la palabra sin posibles correcciones\n",
    "        return no_correction_at_all\n",
    "\n",
    "def spell_check_sentence(sentence):\n",
    "    ''' Busca las posibles correcciones que podemos hacer a una frase\n",
    "        \n",
    "        Parámetro\n",
    "        ---------\n",
    "        sentence : str\n",
    "             Frase a corregir\n",
    "    '''\n",
    "    lower_cased_sentence = sentence.lower()\n",
    "    stripped_sentence = list(map(lambda x : x.strip('.,?¿'), lower_cased_sentence.split()))\n",
    "    checked = map(spell_check_word, stripped_sentence)\n",
    "    checked_words=list(checked)\n",
    "    return ' '.join(checked_words)\n",
    "\n",
    "def spell_check_word(word):\n",
    "    ''' Busca la mejor corrección que podemos hacer a una palabra\n",
    "    \n",
    "        En este caso el criterio que se sigue es el de más frecuencia\n",
    "        \n",
    "        Parámetro\n",
    "        ---------\n",
    "        word : str\n",
    "             Palabra a consultar\n",
    "    '''\n",
    "    return max(possible_corrections(word), key=language_model)\n",
    "\n",
    "def language_model(word):\n",
    "    ''' Nos dice la frecuencia con la cual pasa una palabra candidata a ser la corrección\n",
    "        \n",
    "        Parámetro\n",
    "        ---------\n",
    "        word : str\n",
    "             Palabra a consultar\n",
    "    '''\n",
    "    return WORDS_INDEX.get(word, 0)\n",
    "\n",
    "def filter_real_words(words):\n",
    "    ''' Busca determina cuales palabras de una lista son reales verificando si está en el\n",
    "        diccionario\n",
    "        \n",
    "        Parámetro\n",
    "        ---------\n",
    "        words : list\n",
    "             Palabras a consultar\n",
    "    '''\n",
    "    return set(word for word in words if word in WORDS_INDEX)\n",
    "\n",
    "def one_length_edit(word):\n",
    "    '''Función no alterda por el ataque'''\n",
    "    \n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "    removals_of_one_letter = []\n",
    "    \n",
    "    for left, right in splits:\n",
    "        if right:\n",
    "            removals_of_one_letter.append(left + right[1:])\n",
    "            \n",
    "    two_letters_transposes = []\n",
    "    \n",
    "    for left, right in splits:\n",
    "        if len(right) > 1:\n",
    "            two_letters_transposes.append(left + right[1] + right[0] + right[2:])\n",
    "            \n",
    "    one_letter_replaces = []\n",
    "    \n",
    "    for left, right in splits:\n",
    "        if right:\n",
    "            for c in LETTERS:\n",
    "                one_letter_replaces.append(left + c + right[1:])\n",
    "                \n",
    "    one_letter_insertions = []\n",
    "    \n",
    "    for left, right in splits:\n",
    "        for c in LETTERS:\n",
    "            one_letter_insertions.append(left + c + right)\n",
    "    \n",
    "    one_length_editions = removals_of_one_letter + two_letters_transposes + one_letter_replaces + one_letter_insertions\n",
    "    \n",
    "    return list(set(one_length_editions))\n",
    "\n",
    "\n",
    "\n",
    "def two_lenght_edit(word):\n",
    "    '''Función no alterda por el ataque'''\n",
    "    return [e2 for e1 in one_length_edit(word) for e2 in one_length_edit(e1)]\n",
    "\n",
    "def test_spell_check_sentence():\n",
    "\n",
    "    sentence = 'fabor guardar cilencio para no molestar'\n",
    "    assert 'favor guardar silencio para no molestar' == spell_check_sentence(sentence) \n",
    "\n",
    "    sentence = 'un lgar para la hopinion'\n",
    "    assert 'un lugar para la opinión' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'el Arebol del día'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'el arrebol del día' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'Rezpeto por la educasión'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'respeto por la educación' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'RTe encanta conduzir'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'te encanta conducir' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'HOy ay karne azada frezca siga pa dentro'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'hoy ay carne azada fresca siga la dentro' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'En mi ezcuela no enseñan a escrivir ni a ler'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'en mi escuela no enseñan a escribir ni a le' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'él no era una persona de fiar pues era un mentirozo'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'él no era una persona de fiar pues era un mentiroso' == spell_check_sentence(sentence)\n",
    "test_spell_check_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Solución 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "favor guardar silencio para no molestar\n",
      "un lugar para la opinión\n",
      "el Arrebol del día\n",
      "Respeto por la educación\n",
      "Te encanta conducir\n",
      "Hoy ay carne azada fresca siga la dentro\n",
      "En mi escuela no enseñan a escribir ni a le\n",
      "él no era una persona de fiar pues era un mentiroso\n",
      "Él, no era una persona de fiar, ¡pues era un mentiroso!, lo saber el 30% de la gente\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import ascii_lowercase\n",
    "\n",
    "\n",
    "\n",
    "def fetch_words(read_mode):\n",
    "    '''Función no alterda por el ataque'''\n",
    "    '''Agregué la opción `enconding=\"utf8\"`'''\n",
    "\n",
    "    words_from_dictionary = [ word.strip() for word in open('words.txt', encoding=\"utf8\").readlines() ]\n",
    "    words_from_books = re.findall(r'\\w+', open('BOOKS.txt', read_mode, encoding=\"utf8\").read())\n",
    "    \n",
    "    return words_from_dictionary + words_from_books\n",
    "\n",
    "# El archivo se había abierto como escritura, pero se debe abrir en modo lectura.\n",
    "WORDS = fetch_words('r') #<<<---\n",
    "LETTERS = list(ascii_lowercase) + ['ñ', 'á', 'é', 'í', 'ó', 'ú']\n",
    "SYMBOLS = ',.-¡!¿?#$@&%`´'\n",
    "OPENSYMBOL = \"¡¿`\"\n",
    "CLOSESYMBOL = \"!?´\"\n",
    "OTHERSYMBOL = ',.-#$@&%`´'\n",
    "# Definimos un índice de palabras en el cual guardamos la frecuencia con la que se utiliza en nuestros\n",
    "# textos de referencia\n",
    "WORDS_INDEX = {}\n",
    "\n",
    "# Las ordenes estaban mal y/o al reves.\n",
    "for word in WORDS:\n",
    "    if word in WORDS_INDEX:\n",
    "        WORDS_INDEX.update({word:WORDS_INDEX[word]+ 1})\n",
    "    else:\n",
    "        WORDS_INDEX[word] = 1\n",
    "\n",
    "for symbol in SYMBOLS:\n",
    "    WORDS_INDEX[symbol]=1\n",
    "    \n",
    "def possible_corrections(word):\n",
    "    if filter_real_numbers(word):\n",
    "        return filter_real_numbers(word)\n",
    "    cap=word[0].isupper()\n",
    "    lword=word.lower()\n",
    "    single_word_possible_corrections = filter_real_words([lword])\n",
    "    one_length_edit_possible_corrections = filter_real_words(one_length_edit(lword))\n",
    "    two_lenght_edit_possible_corrections = filter_real_words(two_lenght_edit(lword))\n",
    "    no_correction_at_all = word\n",
    "    if single_word_possible_corrections:\n",
    "        if cap:\n",
    "            return {option.title() for option in single_word_possible_corrections}\n",
    "        return single_word_possible_corrections\n",
    "    elif one_length_edit_possible_corrections:\n",
    "        if cap:\n",
    "            return {option.title() for option in one_length_edit_possible_corrections}\n",
    "        return one_length_edit_possible_corrections\n",
    "    elif two_lenght_edit_possible_corrections:\n",
    "        if cap:\n",
    "            return {option.title() for option in two_lenght_edit_possible_corrections}\n",
    "        return two_lenght_edit_possible_corrections\n",
    "    else:\n",
    "        return no_correction_at_all\n",
    "    \n",
    "def spell_check_sentence(sentence):\n",
    "    stripped_sentence = correct_split(sentence)#list(map(lambda x : x, lower_cased_sentence.split()))\n",
    "    checked = map(spell_check_word, stripped_sentence) #<<-- Ojo que cambié ese filter por map\n",
    "    checked_words=list(checked)\n",
    "    return correct_join(checked_words)#' '.join(checked_words)\n",
    "\n",
    "def spell_check_word(word):\n",
    "    cap=word[0].isupper()\n",
    "    options=possible_corrections(word.lower())\n",
    "    res=max(options, key=language_model)\n",
    "    if word.replace(\"a\",\"á\") in options:\n",
    "        return word.replace(\"a\",\"á\")\n",
    "    if word.replace(\"e\",\"é\") in options:\n",
    "        return word.replace(\"e\",\"é\")\n",
    "    if word.replace(\"i\",\"í\") in options:\n",
    "        return word.replace(\"i\",\"í\")\n",
    "    if word.replace(\"o\",\"ó\") in options:\n",
    "        return word.replace(\"o\",\"ó\")\n",
    "    if word.replace(\"u\",\"ú\") in options:\n",
    "        return word.replace(\"u\",\"ú\")\n",
    "    if not cap:\n",
    "        return res\n",
    "    else:\n",
    "        return res.title()\n",
    "\n",
    "#def spell_check_word(word):\n",
    "#    cap=word[0].isupper()\n",
    "#    if not cap:\n",
    "#        return max(possible_corrections(word.lower()), key=language_model)\n",
    "#    else:\n",
    "#        return max(possible_corrections(word.lower()), key=language_model).title()\n",
    "    \n",
    "def language_model(word):\n",
    "    return WORDS_INDEX.get(word, 0)\n",
    "\n",
    "def filter_real_words(words):\n",
    "    return set(word for word in words if word.lower() in WORDS_INDEX)\n",
    "\n",
    "def filter_real_numbers(word):\n",
    "    try:\n",
    "        temp=int(word)\n",
    "        return set([word])\n",
    "    except:\n",
    "        return set()\n",
    "\n",
    "def one_length_edit(word):\n",
    "    '''Función no alterda por el ataque'''\n",
    "    \n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    #print(splits)\n",
    "    removals_of_one_letter = []\n",
    "    \n",
    "    for left, right in splits:\n",
    "        if right:\n",
    "            removals_of_one_letter.append(left + right[1:])\n",
    "            \n",
    "    two_letters_transposes = []\n",
    "    \n",
    "    for left, right in splits:\n",
    "        if len(right) > 1:\n",
    "            two_letters_transposes.append(left + right[1] + right[0] + right[2:])\n",
    "            \n",
    "    one_letter_replaces = []\n",
    "    \n",
    "    for left, right in splits:\n",
    "        if right:\n",
    "            for c in LETTERS:\n",
    "                one_letter_replaces.append(left + c + right[1:])\n",
    "                \n",
    "    one_letter_insertions = []\n",
    "    \n",
    "    for left, right in splits:\n",
    "        for c in LETTERS:\n",
    "            one_letter_insertions.append(left + c + right)\n",
    "    \n",
    "    one_length_editions = removals_of_one_letter + two_letters_transposes + one_letter_replaces + one_letter_insertions\n",
    "    \n",
    "    return list(set(one_length_editions))\n",
    "\n",
    "def correct_split(sentence):\n",
    "    \"\"\" Esta función crea una lista que divide la frase ubicando los signos aparte\n",
    "        de las palabras\n",
    "        \n",
    "        Parametros\n",
    "        ----------\n",
    "        sentence: str\n",
    "                La frase que se desea partir\n",
    "    \"\"\"\n",
    "    stripped = list(map(lambda x : x, sentence.split()))\n",
    "    stripped_sentence=[]\n",
    "    for word in stripped:\n",
    "        if any(car in word for car in SYMBOLS):\n",
    "            stripped_sentence.extend(word_split(word))\n",
    "        else:\n",
    "            stripped_sentence.append(word)\n",
    "    return stripped_sentence\n",
    "\n",
    "def word_split(word):\n",
    "    punt=SYMBOLS\n",
    "    if not any(p in word for p in punt):\n",
    "        return [word]\n",
    "    tw=word\n",
    "    carry=[]\n",
    "    while(len(punt)>0):\n",
    "        if(tw.find(punt[0])!=-1):\n",
    "            ri=tw[:tw.find(punt[0])]\n",
    "            c=tw[tw.find(punt[0])]\n",
    "            rd=tw[tw.find(punt[0])+1:]\n",
    "            if(len(ri)!=0):\n",
    "                carry.extend(word_split(ri))\n",
    "            carry.append(c)\n",
    "            if(len(rd)!=0):\n",
    "                carry.extend(word_split(rd))\n",
    "            break\n",
    "        punt=punt[1:]\n",
    "    return carry\n",
    "\n",
    "def correct_join(words):\n",
    "    \"\"\" Esta función crea la frase con el formato correcto de las palabras\n",
    "    \n",
    "        Parametro\n",
    "        ---------\n",
    "        words: list\n",
    "             Las palabras que se desean conectar, puede contener signos de puntuación\n",
    "    \"\"\"\n",
    "    sentence=\"\"\n",
    "    for i in range(len(words)):\n",
    "        if any(car in words[i] for car in OPENSYMBOL):\n",
    "            sentence+=words[i]\n",
    "        elif any(car in words[i] for car in CLOSESYMBOL):\n",
    "            if len(sentence)>0:\n",
    "                sentence=sentence[:-1]+words[i]\n",
    "        elif not any(car in words[i] for car in OTHERSYMBOL):\n",
    "            sentence+=words[i]\n",
    "            if i<len(words)-1:\n",
    "                sentence+=\" \"\n",
    "        elif any(car in words[i] for car in OTHERSYMBOL):\n",
    "            if len(sentence)!=0:\n",
    "                if any(car in words[i-1] for car in CLOSESYMBOL):\n",
    "                    sentence+=words[i]\n",
    "                else:\n",
    "                    sentence=sentence[:-1]+words[i]\n",
    "                if i<len(words)-1:\n",
    "                    sentence+=\" \"\n",
    "    return sentence\n",
    "\n",
    "def two_lenght_edit(word):\n",
    "    '''Función no alterda por el ataque'''\n",
    "    return [e2 for e1 in one_length_edit(word) for e2 in one_length_edit(e1)]\n",
    "\n",
    "def test_spell_check_sentence():\n",
    "\n",
    "    sentence = 'fabor guardar cilencio para no molestar'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'favor guardar silencio para no molestar' == spell_check_sentence(sentence) \n",
    "\n",
    "    sentence = 'un lgar para la hopinion'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'un lugar para la opinión' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'el Arebol del día'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'el Arrebol del día' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'Rezpeto por la educasión'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'Respeto por la educación' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'RTe encanta conduzir'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'Te encanta conducir' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'HOy ay karne azada frezca siga pa dentro'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'Hoy ay carne azada fresca siga la dentro' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'En mi ezcuela no enseñan a escrivir ni a ler'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'En mi escuela no enseñan a escribir ni a le' == spell_check_sentence(sentence)\n",
    "\n",
    "    sentence = 'él no era una persona de fiar pues era un mentirozo'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'él no era una persona de fiar pues era un mentiroso' == spell_check_sentence(sentence)\n",
    "    \n",
    "    sentence = '%Él, no era una persona de fiar, ¡pues era un mentirozo!, lo sabe el 30% de la gente'\n",
    "    print(spell_check_sentence(sentence))\n",
    "    assert 'Él, no era una persona de fiar, ¡pues era un mentiroso!, lo saber el 30% de la gente' == spell_check_sentence(sentence)\n",
    "test_spell_check_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'opinión'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_check_sentence(\"hopinion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440502"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORDS_INDEX[\"el\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
